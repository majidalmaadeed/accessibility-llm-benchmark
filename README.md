# LLM Accessibility Benchmark Testing

## Overview

This project provides a systematic evaluation framework for testing Large Language Models' capabilities in detecting and remediating accessibility violations in graphical user interface code. The benchmark evaluates model performance across varying complexity levels (basic, moderate, advanced) and multiple frameworks (web, mobile, desktop), focusing on WCAG compliance, solution quality, and real-world applicability of generated fixes.

## Benchmark Results

| Model | Detection Rate | Solution Quality | WCAG Compliance | Overall Score | Status |
|-------|---------------|------------------|-----------------|---------------|--------|
| **Claude Sonnet 4** | - | - | - | - | üîÑ Testing |
| **Claude Opus 4.1** | - | - | - | - | ‚è≥ Pending |
| **Claude Haiku 3.5** | - | - | - | - | ‚è≥ Pending |
| **GPT-5** | - | - | - | - | ‚è≥ Pending |
| **GPT-4o** | - | - | - | - | ‚è≥ Pending |
| **GPT-4o Mini** | - | - | - | - | ‚è≥ Pending |
| **Gemini 2.5 Pro** | - | - | - | - | ‚è≥ Pending |
| **Gemini 2.5 Flash** | - | - | - | - | ‚è≥ Pending |
| **Gemini 2.0 Flash** | - | - | - | - | ‚è≥ Pending |
| **DeepSeek-V3** | - | - | - | - | ‚è≥ Pending |
| **DeepSeek-V3.1** | - | - | - | - | ‚è≥ Pending |
| **DeepSeek-R1** | - | - | - | - | ‚è≥ Pending |
| **Llama 4** | - | - | - | - | ‚è≥ Pending |
| **Qwen 3** | - | - | - | - | ‚è≥ Pending |

*Results will be updated as testing progresses. Scores range from 0-100.*

## Project Structure
